{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HOMEWORK 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) How would you define Machine Learning?\n",
    "\n",
    "Makine öğrenimi,yapay zeka (AI) dalıdır. Makine öğrenimi, verilerden öğrenilenler ile zaman içinde doğruluğunu artıran uygulamalar oluşturmaya çalışılır.\n",
    "\n",
    "Makine öğreniminde, algoritmalar, yeni verilere dayalı kararlar ve tahminler yapmak için büyük miktarda verideki kalıpları ve özellikleri bulmak üzere istatistiki bilgiler ışığında eğitilir. Algoritma ne kadar iyi olursa, daha fazla veri işledikçe kararlar ve tahminler o kadar doğru olacaktır.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) What are the differences between Supervised and Unsupervised Learning? Specify example 3 algorithms for each of these.\n",
    "\n",
    " * Denetimli öğrenme sürecinde hem girdi hem de çıktı verileri kullanılır. Denetimsiz öğrenme sürecinde ise sadece girdi verileri kullanılır çıktı verileri kullanılmaz. \n",
    " * Denetimli öğrenme basit öğrenme yöntemidir, denetimsiz öğrenme ise daha kompleks öğrenme methodudur.\n",
    " * Denetimli öğrenme, linear ve logistıc regression, random forest, ve classification trees algoritmaları kullanır. Denetimsiz öğrenme ise cluster algorithms, K-means ve hierarchical clustering gibi verileri farklı kategorilere ayırmaya yarayan algoritmaları kullanır.\n",
    " * Denetimli öğrenme, önceki deneyimlerden veri toplayıp, bir veri çıktısı oluşturmamızı sağlar. Denetimsiz makine öğrenimi ise verilerdeki her türlü bilinmeyen kalıbı bulmamıza yardımcı olur.\n",
    " \n",
    " Denetimli Öğrenme Algoritmaları:\n",
    " 1- Linear Regression,\n",
    " 2- Logıstıc Regression,\n",
    " 3- Classification Trees\n",
    " \n",
    " Denetimsiz Öğrenme Algoritmaları:\n",
    " 1- Cluster Algorithms, \n",
    " 2- K-means,\n",
    " 3- Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3)What are the test and validation set, and why would you want to use them?\n",
    "\n",
    "   Veri seti üzerinde tahmin yapacak olan modelimizi geliştirmek için öncelikle train set ve test set olmak üzere 2 parçaya ayırmamız gerekir. Amacımız modelimizi train seti üzerinde geliştirerek test seti üzerinde mümkün olduğu kadar doğru tahmini yapabilmesi olacaktır. Bu nedenle bu ayrımı yaparken veri setimizin büyük çoğunluğunu Train Seti oluşturması gerekir. (örn: 70,30 - 80,20) \n",
    "   \n",
    "   Validatiıon veri seti train seti içerisinden seçilir. Train seti önce validation seti üzerinde eğitilir ve en optimal algoritma bulunmaya çalışılır. Optimal algoritma bulunduktan sonra test seti üzerinde gerçek veriler ile train seti üzerinde eğittiğimiz modelimizin yaptığı tahminler karsılastırılır ve modelimizin tahmin performansı ölçülür.\n",
    "   \n",
    "   Validation seti kullanmamızın nedeni modelimizin kullanacağı en optimal parametreleri ve ağırlıkları bulmaktır. Peki biz neden validation setine ihtiyaç duyarız? Çünkü, büyük veri tiplerinde sürekli train setleri üzerinde çalışılamayacağı için küçük bir veri alınarak validation seti kullanmak uygun bir yöntem olacaktır."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4)What are the main preprocessing steps? Explain them in detail. Why we need to prepare our data?\n",
    "\n",
    "\n",
    "Oluşturacağımız modelimizin veri seti üzerinde doğru tahmin yapmasında belki de en önemli ksıım olan veri ön işleme adımında yaptıklarımızı 4 ana madde ile açıklayabiliriz\n",
    "\n",
    "   1- Data Cleaning (Veri Temizleme) : Bu kısımda eksik, aykırı veya tutarsız verileri tespit edip her biri için ayrı temizleme tekniklerini kullanarak problem setine uygun bir şekilde veri setini güncellememiz gerekmektedir.\n",
    "   \n",
    "   2- Data Integration (Veri Entegrasyonu) : Veri entegrasyonu ile veri bütünlüğünü korumak adına tekrar edilen veriler ve veri uyuşmazlığı tespiti gibi yaklaşımlar aranır.\n",
    "   \n",
    "   3- Data Reduction (Veri İndirgeme) : Veri indirgemenin amacı, veri setinin orijinalin bütünlüğünü korurken, hacim olarak daha küçük olan veri setinin yoğunlaştırılmış bir temsiline sahip olmaktır.\n",
    "   \n",
    "   4- Data Transformation (Veri Dönüşümü) : Veri dönüşümü, verileri veri modellemeye uygun forma dönüştürmektir.\n",
    "\n",
    "Veri ön işleme adımlarının, veri seti modelinin performansının artırılması için kesinlikle titizlikle üzerinde durulması gerekir. Aksi halde veri setinde ki hatalı girdiler bizim modelimizde hatalı çıktı elde etmemize neden olabilir. Bu aşamada verilerin bize ne anlatmak istediğinin anlaşılması, anlamlı veri analizinin yapılabilmesi ve hangi veri ön işleme türünün kullanılması gerektiğinin belirlenmesi bizi başarılı ve kaliteli bir modele götürecektir.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  How you can explore and analyse countionus and discrete variables?\n",
    "\n",
    "Discrete değişkenler değerlerinin sayılarak elde edilebildiği değişkenlerdir. Continuous değişkenler ise bir şeyi ölçen rastgele değişkenlerdir. Continuous değişkenler belirli bir aralık veya süreklilikteki herhangi bir değeri alabilir.\n",
    "\n",
    "Discrete değişkenler 'int' tipinde olur, continuous değişkenler ise hemen hemen her sayısal değeri alabilir ve genelde 'float' tipinde olur.\n",
    "\n",
    "Discrete değişkenlere örnek olarak bir mağazaya gelen müşteri sayısını örnek verebiliriz.\n",
    "Continuous değişkenlere örnek olarak bir şehrin günlük sıcaklık değerleri olabilir. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse the plot given below. (What is the plot and variable type, check the distribution and make comment about how you can preproccess it.)\n",
    "\n",
    "Grafiğin çizim türü histogram türünde bir çizimdir.\n",
    "Değişkenlerin ise continuous değişken türü olduğunu görebiliyoruz.\n",
    "Dağıtıma bakıldığında ilk göze çarpan durumun 2 farklı tepe noktası olduğu anlaşılmaktadır. Bu durum petal width özelliğinin ayırt edici bir özellik olabileceğini bize erkenden söyleyebilir.Grafikte normal bir dağılım söz konusu değildir.Bu yüzden veri ön işleme kısmında scaling yöntemlerinden Standardization yaklaşımının kullanılması uygun olacaktır.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
